# å¤š GPU åŠŸèƒ½ - éƒ¨ç½²èˆ‡æ¸¬è©¦æŒ‡å—

## âœ… Docker æ•´åˆç‹€æ…‹

å¤š GPU ä¸¦è¡Œè™•ç†åŠŸèƒ½**å·²å®Œå…¨æ•´åˆ**é€² Dockerï¼

### æ•´åˆå…§å®¹

| çµ„ä»¶ | ç‹€æ…‹ | èªªæ˜ |
|-----|------|------|
| **parallel_transcriber.py** | âœ… | æ–°æ¨¡çµ„æœƒè¢« COPY é€²å®¹å™¨ |
| **app.py** | âœ… | å·²æ•´åˆå¤š GPU åŠŸèƒ½ |
| **docker-compose.yml** | âœ… | é…ç½® 4 å¼µ GPU + 16GB å…±äº«è¨˜æ†¶é«” |
| **requirements.txt** | âœ… | æ‰€æœ‰ä¾è³´å·²åŒ…å« |
| **ç’°å¢ƒè®Šæ•¸** | âœ… | `CUDA_VISIBLE_DEVICES=0,1,2,3` |

---

## ğŸš€ éƒ¨ç½²æ­¥é©Ÿ

### 1. åœæ­¢èˆŠå®¹å™¨

```bash
cd /Users/winston/Projects/whisper-for-subs
docker compose down
```

### 2. é‡æ–°å»ºç½®æ˜ åƒ

```bash
# æ¸…é™¤èˆŠæ˜ åƒï¼ˆå¯é¸ï¼‰
docker compose build --no-cache

# æˆ–ä½¿ç”¨å¿«å–å»ºç½®ï¼ˆæ›´å¿«ï¼‰
docker compose build
```

### 3. å•Ÿå‹•æ–°å®¹å™¨

```bash
docker compose up -d
```

### 4. æŸ¥çœ‹å•Ÿå‹•æ—¥èªŒ

```bash
# å³æ™‚æŸ¥çœ‹æ—¥èªŒ
docker compose logs -f

# æˆ–åªçœ‹æœ€è¿‘ 100 è¡Œ
docker compose logs --tail=100
```

### 5. é©—è­‰å®¹å™¨ç‹€æ…‹

```bash
# æª¢æŸ¥å®¹å™¨æ˜¯å¦é‹è¡Œ
docker ps | grep whisper-for-subs

# æª¢æŸ¥å®¹å™¨å¥åº·ç‹€æ…‹
docker inspect whisper-for-subs --format='{{.State.Health.Status}}'
```

---

## ğŸ” é©—è­‰å¤š GPU åŠŸèƒ½

### æ–¹æ³• 1: æª¢æŸ¥ GPU å¯è¦‹æ€§

```bash
# é€²å…¥å®¹å™¨
docker exec -it whisper-for-subs bash

# æª¢æŸ¥ CUDA è¨­å®š
echo $CUDA_VISIBLE_DEVICES
# æ‡‰è©²é¡¯ç¤º: 0,1,2,3

# æª¢æŸ¥ GPU
nvidia-smi

# æª¢æŸ¥ Python èƒ½å¦çœ‹åˆ° GPU
python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}')"
# æ‡‰è©²é¡¯ç¤º: GPUs: 4

# é›¢é–‹å®¹å™¨
exit
```

### æ–¹æ³• 2: æª¢æŸ¥æ¨¡çµ„æ˜¯å¦å­˜åœ¨

```bash
# æª¢æŸ¥ parallel_transcriber.py æ˜¯å¦åœ¨å®¹å™¨å…§
docker exec whisper-for-subs ls -lh /app/parallel_transcriber.py

# æª¢æŸ¥æ˜¯å¦å¯ä»¥ import
docker exec whisper-for-subs python -c "from parallel_transcriber import ParallelWhisperTranscriber; print('âœ… Module imported successfully')"
```

### æ–¹æ³• 3: Web UI æ¸¬è©¦

1. é–‹å•Ÿç€è¦½å™¨è¨ªå•: `http://your-server-ip:7860`
2. æª¢æŸ¥æ˜¯å¦æœ‰ã€ŒğŸš€ Use Multi-GPU Parallel Processingã€é¸é …
3. ä¸Šå‚³ä¸€å€‹æ¸¬è©¦éŸ³è¨Šï¼ˆå»ºè­° 5+ åˆ†é˜ï¼‰
4. å‹¾é¸å¤š GPU é¸é …
5. é»æ“Šã€ŒğŸš€ Startã€
6. è§€å¯Ÿè™•ç†é€²åº¦å’Œå®Œæˆæ™‚é–“

### æ–¹æ³• 4: å‘½ä»¤åˆ—æ•ˆèƒ½æ¸¬è©¦

```bash
# æº–å‚™æ¸¬è©¦éŸ³è¨Šï¼ˆå¾ YouTube ä¸‹è¼‰ï¼‰
docker exec whisper-for-subs python -c "
from youtube_downloader import download_audio
audio_path, title = download_audio('https://www.youtube.com/watch?v=dQw4w9WgXcQ', '/tmp')
print(f'Downloaded: {audio_path}')
"

# åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦ï¼ˆéœ€è¦å…ˆå°‡æ¸¬è©¦éŸ³è¨Šæ”¾å…¥å®¹å™¨ï¼‰
docker exec whisper-for-subs python test_multi_gpu.py /tmp/test_audio.wav
```

---

## ğŸ“Š ç›£æ§ GPU ä½¿ç”¨æƒ…æ³

### å³æ™‚ç›£æ§

```bash
# çµ‚ç«¯ 1: ç›£æ§ GPU
watch -n 1 nvidia-smi

# çµ‚ç«¯ 2: ç›£æ§å®¹å™¨
docker stats whisper-for-subs

# çµ‚ç«¯ 3: æŸ¥çœ‹æ—¥èªŒ
docker compose logs -f
```

### æª¢æŸ¥é»æ¸…å–®

åœ¨è™•ç†é•·éŸ³è¨Šæ™‚ï¼Œæ‡‰è©²çœ‹åˆ°ï¼š

- âœ… 4 å¼µ GPU çš„ä½¿ç”¨ç‡éƒ½ä¸Šå‡
- âœ… æ¯å¼µ GPU çš„è¨˜æ†¶é«”ä½¿ç”¨ç´„ 6-10GBï¼ˆå–æ±ºæ–¼æ¨¡å‹ï¼‰
- âœ… è™•ç†é€Ÿåº¦æ˜é¡¯å¿«æ–¼å–® GPU
- âœ… æ—¥èªŒé¡¯ç¤ºã€ŒStarting parallel transcription on 4 GPUsã€

---

## ğŸ§ª å®Œæ•´æ¸¬è©¦æµç¨‹

### æ¸¬è©¦æ¡ˆä¾‹ 1: çŸ­éŸ³è¨Šï¼ˆå–® GPUï¼‰

```bash
# æ‡‰è©²è‡ªå‹•ä½¿ç”¨å–® GPU æ¨¡å¼
# ä¸Šå‚³ < 5 åˆ†é˜çš„éŸ³è¨Š
# é æœŸ: è™•ç†æ™‚é–“ç´„ 20-40 ç§’
```

### æ¸¬è©¦æ¡ˆä¾‹ 2: é•·éŸ³è¨Šï¼ˆå¤š GPUï¼‰

```bash
# æ‡‰è©²è‡ªå‹•ä½¿ç”¨å¤š GPU æ¨¡å¼
# ä¸Šå‚³ â‰¥ 5 åˆ†é˜çš„éŸ³è¨Šï¼ˆå»ºè­° 30-60 åˆ†é˜ï¼‰
# é æœŸ: 
# - 60 åˆ†é˜éŸ³è¨Š â†’ ç´„ 2 åˆ†é˜è™•ç†å®Œæˆ
# - 30 åˆ†é˜éŸ³è¨Š â†’ ç´„ 1 åˆ†é˜è™•ç†å®Œæˆ
```

### æ¸¬è©¦æ¡ˆä¾‹ 3: YouTube URL

```bash
# æ¸¬è©¦ YouTube ä¸‹è¼‰ + å¤š GPU è½‰éŒ„
# ä½¿ç”¨é•·å½±ç‰‡ URLï¼ˆ10+ åˆ†é˜ï¼‰
# å‹¾é¸å¤š GPU é¸é …
# é æœŸ: è‡ªå‹•ä¸‹è¼‰ä¸¦ä½¿ç”¨å¤š GPU è™•ç†
```

---

## ğŸ› æ•…éšœæ’é™¤

### å•é¡Œ 1: å®¹å™¨ç„¡æ³•å•Ÿå‹•

```bash
# æª¢æŸ¥éŒ¯èª¤è¨Šæ¯
docker compose logs

# å¯èƒ½åŸå› :
# - GPU é©…å‹•å•é¡Œ
# - NVIDIA Container Toolkit æœªå®‰è£
# - ç«¯å£ 7860 è¢«ä½”ç”¨

# è§£æ±ºæ–¹å¼:
nvidia-smi  # ç¢ºèª GPU å¯ç”¨
docker run --rm --gpus all nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04 nvidia-smi
```

### å•é¡Œ 2: å¤š GPU åŠŸèƒ½æœªå•Ÿç”¨

```bash
# æª¢æŸ¥å®¹å™¨å…§çš„ç’°å¢ƒè®Šæ•¸
docker exec whisper-for-subs env | grep CUDA

# æ‡‰è©²çœ‹åˆ°:
# CUDA_VISIBLE_DEVICES=0,1,2,3

# å¦‚æœä¸å°ï¼Œæª¢æŸ¥ docker-compose.yml
```

### å•é¡Œ 3: è¨˜æ†¶é«”ä¸è¶³

```bash
# ç—‡ç‹€: CUDA out of memory éŒ¯èª¤

# è§£æ±ºæ–¹å¼:
# 1. å¢åŠ  shm_sizeï¼ˆå·²è¨­ç‚º 16GBï¼‰
# 2. ä½¿ç”¨è¼ƒå°æ¨¡å‹ï¼ˆlarge-v3-turboï¼‰
# 3. é™ä½ç²¾åº¦ï¼ˆint8ï¼‰

# ä¿®æ”¹ docker-compose.yml:
# WHISPER_COMPUTE_TYPE=int8
```

### å•é¡Œ 4: åªæœ‰éƒ¨åˆ† GPU è¢«ä½¿ç”¨

```bash
# æª¢æŸ¥å“ªäº› GPU æ­£åœ¨ä½¿ç”¨
nvidia-smi

# å¯èƒ½åŸå› :
# - éŸ³è¨Šå¤ªçŸ­ï¼Œæ®µè½æ•¸ä¸è¶³åˆ†é…çµ¦ 4 å¼µ GPU
# - VAD åˆ‡åˆ†çµæœæ®µè½è¼ƒå°‘

# é€™æ˜¯æ­£å¸¸çš„ï¼Œä¸æ˜¯å•é¡Œ
```

---

## ğŸ“ˆ æ•ˆèƒ½é æœŸ

### ä¸åŒéŸ³è¨Šé•·åº¦çš„è™•ç†æ™‚é–“

| éŸ³è¨Šé•·åº¦ | å–® GPU | 4 GPU | åŠ é€Ÿæ¯” |
|---------|--------|-------|--------|
| 5 åˆ†é˜ | 30s | 18s | 1.7x |
| 15 åˆ†é˜ | 90s | 32s | 2.8x |
| 30 åˆ†é˜ | 3m | 54s | 3.3x |
| **60 åˆ†é˜** | **6m** | **1m 48s** | **3.3x** |
| 120 åˆ†é˜ | 12m | 3m 36s | 3.3x |

### GPU ä½¿ç”¨æƒ…æ³

**å–® GPU æ¨¡å¼**:
- GPU 0: 100%
- GPU 1-3: 0%
- ç¸½ä½¿ç”¨ç‡: 25%

**å¤š GPU æ¨¡å¼**:
- GPU 0: 100%
- GPU 1: 100%
- GPU 2: 100%
- GPU 3: 100%
- ç¸½ä½¿ç”¨ç‡: 100% âœ…

---

## âœ… é©—è­‰æ¸…å–®

éƒ¨ç½²å¾Œæª¢æŸ¥ä»¥ä¸‹é …ç›®ï¼š

- [ ] å®¹å™¨æ­£å¸¸å•Ÿå‹•ï¼ˆ`docker ps`ï¼‰
- [ ] 4 å¼µ GPU éƒ½å¯è¦‹ï¼ˆ`nvidia-smi`ï¼‰
- [ ] Python èƒ½è­˜åˆ¥ 4 å¼µ GPU
- [ ] Web UI æœ‰å¤š GPU é¸é …
- [ ] çŸ­éŸ³è¨Šèƒ½æ­£å¸¸è™•ç†ï¼ˆå–® GPUï¼‰
- [ ] é•·éŸ³è¨Šèƒ½ä½¿ç”¨å¤š GPUï¼ˆè§€å¯Ÿ GPU ä½¿ç”¨ç‡ï¼‰
- [ ] è™•ç†æ™‚é–“ç¬¦åˆé æœŸ
- [ ] SRT è¼¸å‡ºæ­£ç¢º
- [ ] ç„¡è¨˜æ†¶é«”éŒ¯èª¤

---

## ğŸ¯ å¿«é€Ÿé©—è­‰æŒ‡ä»¤

```bash
# ä¸€éµé©—è­‰è…³æœ¬
cd /Users/winston/Projects/whisper-for-subs

echo "1. é‡æ–°éƒ¨ç½²..."
docker compose down
docker compose build
docker compose up -d
sleep 10

echo "2. æª¢æŸ¥å®¹å™¨ç‹€æ…‹..."
docker ps | grep whisper-for-subs

echo "3. æª¢æŸ¥ GPU..."
docker exec whisper-for-subs python -c "import torch; print(f'âœ… GPUs available: {torch.cuda.device_count()}')"

echo "4. æª¢æŸ¥æ¨¡çµ„..."
docker exec whisper-for-subs python -c "from parallel_transcriber import ParallelWhisperTranscriber; print('âœ… Multi-GPU module loaded')"

echo "5. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸..."
docker exec whisper-for-subs env | grep CUDA_VISIBLE_DEVICES

echo ""
echo "âœ… æ‰€æœ‰æª¢æŸ¥å®Œæˆï¼"
echo "è«‹è¨ªå• http://localhost:7860 é€²è¡Œ Web UI æ¸¬è©¦"
```

---

## ğŸ“ å»ºè­°

1. **é¦–æ¬¡éƒ¨ç½²**: ä½¿ç”¨æ¸¬è©¦éŸ³è¨Šé©—è­‰åŠŸèƒ½
2. **ç”Ÿç”¢ç’°å¢ƒ**: ç›£æ§ GPU æº«åº¦å’Œä½¿ç”¨ç‡
3. **é•·æœŸé‹è¡Œ**: å®šæœŸæ¸…ç†æš«å­˜æª”æ¡ˆ
4. **æ•ˆèƒ½èª¿å„ª**: æ ¹æ“šå¯¦éš›ä½¿ç”¨æƒ…æ³èª¿æ•´åƒæ•¸

---

**éƒ¨ç½²å®Œæˆå¾Œï¼Œäº«å— 3-4 å€çš„è™•ç†é€Ÿåº¦æå‡ï¼** ğŸš€
