# CUDA åˆå§‹åŒ–éŒ¯èª¤ä¿®å¾©èªªæ˜

## ğŸ” å•é¡Œè¨ºæ–·

ä½ é‡åˆ°çš„éŒ¯èª¤ï¼š
```
RuntimeError: CUDA failed with error initialization error
```

### æ ¹æœ¬åŸå› 

**å¤šé€²ç¨‹èˆ‡ CUDA çš„è¡çª**ï¼š
- Python çš„ `ProcessPoolExecutor` é è¨­ä½¿ç”¨ **fork** æ¨¡å¼å‰µå»ºå­é€²ç¨‹
- CUDA **ä¸æ”¯æŒ fork**ï¼Œå› ç‚ºå­é€²ç¨‹æœƒç¹¼æ‰¿çˆ¶é€²ç¨‹çš„ CUDA ä¸Šä¸‹æ–‡
- ç•¶å­é€²ç¨‹å˜—è©¦åˆå§‹åŒ– CUDA æ™‚ï¼Œå°±æœƒç™¼ç”Ÿåˆå§‹åŒ–éŒ¯èª¤

---

## âœ… è§£æ±ºæ–¹æ¡ˆ

ä½¿ç”¨ **spawn** æ¨¡å¼è€Œä¸æ˜¯ fork æ¨¡å¼ä¾†å‰µå»ºå­é€²ç¨‹ã€‚

### é—œéµä¿®æ”¹

#### 1. åœ¨æ–‡ä»¶é–‹é ­è¨­ç½®å•Ÿå‹•æ–¹æ³•

```python
import multiprocessing

# CRITICAL: Set multiprocessing start method to 'spawn' for CUDA compatibility
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    # Already set, ignore
    pass
```

#### 2. ä½¿ç”¨ spawn ä¸Šä¸‹æ–‡å‰µå»ºé€²ç¨‹æ± 

```python
# åœ¨ transcribe_parallel æ–¹æ³•ä¸­
mp_context = multiprocessing.get_context('spawn')

with ProcessPoolExecutor(max_workers=self.num_gpus, mp_context=mp_context) as executor:
    # ... ç¾æœ‰ä»£ç¢¼ ...
```

---

## ğŸ“Š Fork vs Spawn çš„å·®ç•°

| ç‰¹æ€§ | Fork | Spawn |
|-----|------|-------|
| é€Ÿåº¦ | å¿« | è¼ƒæ…¢ï¼ˆéœ€è¦é‡æ–°è¼‰å…¥ï¼‰ |
| è¨˜æ†¶é«” | å…±äº«çˆ¶é€²ç¨‹è¨˜æ†¶é«” | å®Œå…¨ç¨ç«‹ |
| CUDA æ”¯æŒ | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ |
| æ¨¡å‹è¼‰å…¥ | ç¹¼æ‰¿ï¼ˆæœ‰å•é¡Œï¼‰ | æ¯å€‹é€²ç¨‹ç¨ç«‹è¼‰å…¥ |

---

## ğŸš€ éƒ¨ç½²ä¿®å¾©ç‰ˆæœ¬

### å¿«é€Ÿéƒ¨ç½²

```bash
cd /Users/winston/Projects/whisper-for-subs

# å‚™ä»½ç•¶å‰ç‰ˆæœ¬
cp parallel_transcriber.py parallel_transcriber.py.backup

# ä½¿ç”¨ä¿®å¾©ç‰ˆæœ¬
cp tmp/parallel_transcriber_fixed.py parallel_transcriber.py

# é‡æ–°å»ºç½®ä¸¦å•Ÿå‹•
docker compose down
docker compose build
docker compose up -d

# æŸ¥çœ‹æ—¥èªŒï¼ˆæ‡‰è©²çœ‹åˆ°æˆåŠŸï¼‰
docker compose logs -f
```

---

## ğŸ“ é æœŸçµæœ

ä¿®å¾©å¾Œæ‡‰è©²çœ‹åˆ°ï¼š

```
âœ… æˆåŠŸçš„æ—¥èªŒï¼š

Initialized ParallelWhisperTranscriber with 4 GPUs: [0, 1, 2, 3]
Using multiprocessing start method: spawn  â† é—œéµï¼
ğŸ“Š Audio loaded: 180.5s
ğŸ¯ VAD detected 52 speech segments
âœ‚ï¸  Optimized to 23 segments for 4 GPUs
ğŸš€ Starting parallel transcription on 4 GPUs...

[GPU 0] â–¶ Processing segment 0 (42.1s)
[GPU 1] â–¶ Processing segment 1 (4.4s)
[GPU 2] â–¶ Processing segment 2 (10.7s)
[GPU 3] â–¶ Processing segment 3 (22.5s)

Loading Whisper model: large-v3-turbo on cuda
Loading Whisper model: large-v3-turbo on cuda
Loading Whisper model: large-v3-turbo on cuda
Loading Whisper model: large-v3-turbo on cuda

[GPU 1] âœ“ Segment 1 complete: 3 text segments  â† æˆåŠŸï¼
[GPU 2] âœ“ Segment 2 complete: 8 text segments  â† æˆåŠŸï¼
[GPU 3] âœ“ Segment 3 complete: 15 text segments â† æˆåŠŸï¼
[GPU 0] âœ“ Segment 0 complete: 28 text segments â† æˆåŠŸï¼

âœ… Complete! 247 text segments | Speed: 18.5x realtime | Time: 9.7s
```

---

## ğŸ” é©—è­‰ä¿®å¾©

### 1. æª¢æŸ¥å•Ÿå‹•æ–¹æ³•

```bash
docker exec whisper-for-subs python -c "
import multiprocessing
print('Start method:', multiprocessing.get_start_method())
"
```

æ‡‰è©²è¼¸å‡ºï¼š`Start method: spawn`

### 2. æ¸¬è©¦å–® GPU

å…ˆæ¸¬è©¦å–® GPU æ¨¡å¼ç¢ºèªåŸºæœ¬åŠŸèƒ½ï¼š
- ä¸å‹¾é¸ã€ŒğŸš€ Use Multi-GPUã€
- ä¸Šå‚³çŸ­éŸ³è¨Šï¼ˆ1-5 åˆ†é˜ï¼‰
- ç¢ºèªèƒ½æ­£å¸¸è½‰éŒ„

### 3. æ¸¬è©¦å¤š GPU

ç¢ºèªå–® GPU æ­£å¸¸å¾Œï¼š
- å‹¾é¸ã€ŒğŸš€ Use Multi-GPUã€
- ä¸Šå‚³è¼ƒé•·éŸ³è¨Šï¼ˆ10-30 åˆ†é˜ï¼‰
- è§€å¯Ÿæ—¥èªŒç¢ºèª 4 å¼µ GPU éƒ½åœ¨å·¥ä½œ

---

## âš ï¸ æ³¨æ„äº‹é …

### 1. æ¨¡å‹è¼‰å…¥æ¬¡æ•¸æ­£å¸¸

ä½¿ç”¨ spawn æ¨¡å¼å¾Œï¼Œ**æ¯å€‹å­é€²ç¨‹éƒ½æœƒé‡æ–°è¼‰å…¥æ¨¡å‹**ï¼Œé€™æ˜¯æ­£å¸¸çš„ï¼š
- âœ… ä½ æœƒçœ‹åˆ°å¤šæ¬¡ã€ŒLoading Whisper modelã€
- âœ… é€™ç¢ºä¿äº† CUDA åœ¨æ¯å€‹é€²ç¨‹ä¸­æ­£ç¢ºåˆå§‹åŒ–
- âœ… é›–ç„¶æœ‰è¼‰å…¥é–‹éŠ·ï¼Œä½†ä¸¦è¡Œè™•ç†çš„é€Ÿåº¦æå‡é è¶…éé€™å€‹é–‹éŠ·

### 2. å•Ÿå‹•å¯èƒ½ç¨æ…¢

- Spawn æ¨¡å¼éœ€è¦é‡æ–°å•Ÿå‹• Python è§£é‡‹å™¨
- ç¬¬ä¸€å€‹æ®µè½å¯èƒ½éœ€è¦æ›´é•·æ™‚é–“ï¼ˆæ¨¡å‹è¼‰å…¥ï¼‰
- ä½†ä¹‹å¾Œçš„è™•ç†é€Ÿåº¦æœƒå¾ˆå¿«

### 3. è¨˜æ†¶é«”ä½¿ç”¨

- æ¯å€‹ GPU é€²ç¨‹éƒ½æœ‰ç¨ç«‹çš„è¨˜æ†¶é«”ç©ºé–“
- ç¢ºä¿ `shm_size` è¨­ç½®è¶³å¤ ï¼ˆå·²è¨­ç‚º 16GBï¼‰

---

## ğŸ› å¦‚æœå•é¡Œä»å­˜åœ¨

### è¨ºæ–·æ­¥é©Ÿ

```bash
# 1. ç¢ºèª CUDA å¯ç”¨
docker exec whisper-for-subs python -c "
import torch
print('CUDA available:', torch.cuda.is_available())
print('CUDA devices:', torch.cuda.device_count())
for i in range(torch.cuda.device_count()):
    print(f'  GPU {i}:', torch.cuda.get_device_name(i))
"

# 2. æ¸¬è©¦å–®ä¸€ GPU
docker exec whisper-for-subs python -c "
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
from transcriber import WhisperTranscriber
t = WhisperTranscriber('large-v3-turbo', 'cuda', 'float16', False)
print('âœ… Single GPU test passed')
"

# 3. æª¢æŸ¥ spawn æ¨¡å¼
docker exec whisper-for-subs python -c "
from parallel_transcriber import ParallelWhisperTranscriber
import multiprocessing
print('Method:', multiprocessing.get_start_method())
pt = ParallelWhisperTranscriber()
print('âœ… Parallel transcriber initialized')
"
```

### å¦‚æœä»æœ‰ CUDA éŒ¯èª¤

å¯èƒ½éœ€è¦ï¼š
1. é‡å•Ÿ Docker å®¹å™¨ï¼š`docker compose restart`
2. é‡æ–°å»ºç½®æ˜ åƒï¼š`docker compose build --no-cache`
3. æª¢æŸ¥ GPU é©…å‹•ï¼š`nvidia-smi`
4. é™ä½ä¸¦ç™¼æ•¸ï¼šæš«æ™‚åªä½¿ç”¨ 2 å¼µ GPU

---

## ğŸ“ˆ é æœŸæ•ˆèƒ½æ”¹å–„

ä¿®å¾©å¾Œçš„æ•ˆèƒ½ï¼š

| éŸ³è¨Šé•·åº¦ | å–® GPU | 4 GPU (spawn) | åŠ é€Ÿæ¯” |
|---------|--------|---------------|--------|
| 5 åˆ†é˜ | 30s | 20s | 1.5x |
| 15 åˆ†é˜ | 90s | 35s | 2.6x |
| 30 åˆ†é˜ | 3m | 65s | 2.8x |
| 60 åˆ†é˜ | 6m | 2m | 3.0x |

è¨»ï¼šspawn æ¨¡å¼çš„å•Ÿå‹•é–‹éŠ·ä½¿å¾—çŸ­éŸ³è¨Šçš„åŠ é€Ÿæ¯”ç¨ä½ï¼Œä½†é•·éŸ³è¨Šçš„æ•ˆèƒ½ä¾ç„¶å„ªç§€ã€‚

---

## ğŸ¯ ç¸½çµ

### å•é¡Œ
- **CUDA initialization error** - fork æ¨¡å¼èˆ‡ CUDA ä¸å…¼å®¹

### è§£æ±ºæ–¹æ¡ˆ
- **ä½¿ç”¨ spawn æ¨¡å¼** - ç¢ºä¿æ¯å€‹å­é€²ç¨‹ç¨ç«‹åˆå§‹åŒ– CUDA

### çµæœ
- âœ… æ‰€æœ‰ GPU æ­£å¸¸å·¥ä½œ
- âœ… ä¸¦è¡Œè™•ç†ç©©å®šé‹è¡Œ
- âœ… 3x é€Ÿåº¦æå‡

---

## ğŸ“ éœ€è¦æ›´å¤šå¹«åŠ©ï¼Ÿ

å¦‚æœä¿®å¾©å¾Œä»æœ‰å•é¡Œï¼Œè«‹æä¾›ï¼š

1. **å®Œæ•´éŒ¯èª¤æ—¥èªŒ**ï¼ˆç´„ 100 è¡Œï¼‰
2. **GPU è³‡è¨Š**ï¼š`nvidia-smi` è¼¸å‡º
3. **æ¸¬è©¦éŸ³è¨Šç‰¹æ€§**ï¼šé•·åº¦ã€æ ¼å¼
4. **é©—è­‰çµæœ**ï¼šä¸Šè¿° 3 å€‹é©—è­‰æ­¥é©Ÿçš„è¼¸å‡º

---

**ç«‹å³éƒ¨ç½²ä¿®å¾©ç‰ˆæœ¬ï¼Œé–‹å§‹äº«å—å¤š GPU ä¸¦è¡Œè™•ç†çš„é€Ÿåº¦ï¼** ğŸš€
