# å–® GPU æ¨¡å¼ä¿®å¾© v2 - æ­£ç¢ºç‰ˆæœ¬

## âŒ ä¹‹å‰çš„éŒ¯èª¤

ç¬¬ä¸€æ¬¡ä¿®å¾©ä½¿ç”¨äº† `device="cuda:0"`ï¼Œä½†é€™å°è‡´éŒ¯èª¤ï¼š
```
ValueError: unsupported device cuda:0
```

**åŸå› **ï¼š`faster-whisper` çš„ `WhisperModel` åªæ”¯æŒï¼š
- âœ… `device="cuda"` - ä½¿ç”¨ CUDA
- âœ… `device="cpu"` - ä½¿ç”¨ CPU
- âŒ `device="cuda:0"` - **ä¸æ”¯æŒ**

---

## âœ… æ­£ç¢ºçš„è§£æ±ºæ–¹æ¡ˆ

ä½¿ç”¨ **PyTorch çš„ `torch.cuda.set_device(0)`** ä¾†è¨­ç½®é è¨­ GPUï¼Œè€Œä¸æ˜¯åœ¨ device åƒæ•¸ä¸­æŒ‡å®šã€‚

### ä¿®æ”¹å…§å®¹

```python
import torch  # æ–°å¢ import

def get_transcriber(
    model_size: str = "large-v3",
    use_vad: bool = True,
) -> WhisperTranscriber:
    """Get or create single-GPU transcriber instance (uses only GPU 0)."""
    global transcriber
    
    if transcriber is None or transcriber.model_size != model_size:
        device = os.environ.get("WHISPER_DEVICE", "cuda")
        
        # âœ… ä½¿ç”¨ PyTorch è¨­ç½®é è¨­ GPU
        if device == "cuda" and torch.cuda.is_available():
            torch.cuda.set_device(0)  # è¨­ç½® GPU 0 ç‚ºé è¨­
        
        transcriber = WhisperTranscriber(
            model_size=model_size,
            device=device,  # âœ… ä½¿ç”¨ "cuda" è€Œä¸æ˜¯ "cuda:0"
            compute_type=os.environ.get("WHISPER_COMPUTE_TYPE", "float16"),
            use_vad=use_vad,
        )
    
    return transcriber
```

---

## ğŸš€ éƒ¨ç½²

```bash
cd /Users/winston/Projects/whisper-for-subs

# é‡å»ºå®¹å™¨
docker compose down
docker compose build
docker compose up -d

# æŸ¥çœ‹æ—¥èªŒ
docker compose logs -f
```

---

## âœ… é æœŸçµæœ

### æˆåŠŸçš„æ—¥èªŒ

```
Loading Whisper model: large-v3-turbo on cuda
âœ… æˆåŠŸè¼‰å…¥ï¼
```

**ä¸å†å‡ºç¾**ï¼š
```
âŒ ValueError: unsupported device cuda:0
```

### GPU ä½¿ç”¨æƒ…æ³

**å–® GPU æ¨¡å¼ï¼ˆå–æ¶ˆå‹¾é¸ï¼‰ï¼š**
```bash
$ nvidia-smi
GPU 0: 85% âœ… (åªæœ‰ GPU 0 åœ¨å·¥ä½œ)
GPU 1:  0% âœ…
GPU 2:  0% âœ…
GPU 3:  0% âœ…
```

**å¤š GPU æ¨¡å¼ï¼ˆå‹¾é¸ï¼‰ï¼š**
```bash
$ nvidia-smi
GPU 0: 95% âœ…
GPU 1: 92% âœ…
GPU 2: 88% âœ…
GPU 3: 90% âœ…
```

---

## ğŸ” æŠ€è¡“èªªæ˜

### ç‚ºä»€éº¼ä½¿ç”¨ torch.cuda.set_device(0)ï¼Ÿ

**faster-whisper çš„ API é™åˆ¶**ï¼š
- `WhisperModel` ä½¿ç”¨ `ctranslate2` å¾Œç«¯
- `ctranslate2` çš„ device åƒæ•¸åªæ¥å— `"cuda"` æˆ– `"cpu"`
- ä¸æ”¯æŒ `"cuda:0"` é€™ç¨® PyTorch é¢¨æ ¼çš„æŒ‡å®š

**æ­£ç¢ºçš„æ–¹æ³•**ï¼š
```python
# âœ… æ­£ç¢ºï¼šä½¿ç”¨ PyTorch API è¨­ç½®é è¨­ GPU
torch.cuda.set_device(0)  # è¨­ç½® GPU 0 ç‚ºé è¨­
model = WhisperModel("large-v3", device="cuda")  # æœƒä½¿ç”¨ GPU 0

# âŒ éŒ¯èª¤ï¼šç›´æ¥æŒ‡å®š GPU
model = WhisperModel("large-v3", device="cuda:0")  # ValueError!
```

**å·¥ä½œåŸç†**ï¼š
1. `torch.cuda.set_device(0)` è¨­ç½®ç•¶å‰é€²ç¨‹çš„é è¨­ CUDA è¨­å‚™ç‚º GPU 0
2. ä¹‹å¾Œæ‰€æœ‰çš„ CUDA æ“ä½œï¼ˆåŒ…æ‹¬ faster-whisperï¼‰éƒ½æœƒä½¿ç”¨ GPU 0
3. é€™æ˜¯æ¨™æº–çš„ PyTorch æ–¹å¼ï¼Œç›¸å®¹æ–¼æ‰€æœ‰ä½¿ç”¨ CUDA çš„åº«

---

## ğŸ“Š ä¿®æ”¹æ­·å²

### v1ï¼ˆéŒ¯èª¤ï¼‰
```python
device = "cuda:0"  # âŒ faster-whisper ä¸æ”¯æŒ
transcriber = WhisperTranscriber(device=device)
# ValueError: unsupported device cuda:0
```

### v2ï¼ˆæ­£ç¢ºï¼‰
```python
torch.cuda.set_device(0)  # âœ… è¨­ç½®é è¨­ GPU
device = "cuda"  # âœ… ä½¿ç”¨æ¨™æº–æ ¼å¼
transcriber = WhisperTranscriber(device=device)
# âœ… æˆåŠŸï¼
```

---

## ğŸ¯ é©—è­‰æ­¥é©Ÿ

### 1. æª¢æŸ¥å®¹å™¨æ—¥èªŒ

```bash
docker logs whisper-for-subs 2>&1 | grep -A 5 "Loading Whisper"
```

**æ‡‰è©²çœ‹åˆ°**ï¼š
```
Loading Whisper model: large-v3-turbo on cuda
âœ… æˆåŠŸï¼
```

**ä¸æ‡‰è©²çœ‹åˆ°**ï¼š
```
ValueError: unsupported device cuda:0  # âŒ ä¸æ‡‰è©²å‡ºç¾
```

### 2. æ¸¬è©¦å–® GPU æ¨¡å¼

```bash
# çµ‚ç«¯ 1: ç›£æ§ GPU
watch -n 1 nvidia-smi

# çµ‚ç«¯ 2: è™•ç†éŸ³è¨Š
# 1. è¨ªå• http://localhost:7860
# 2. ä¸Šå‚³éŸ³è¨Š
# 3. **å–æ¶ˆå‹¾é¸** Multi-GPU
# 4. é»æ“Š Start
# 5. ç¢ºèªåªæœ‰ GPU 0 æœ‰è² è¼‰
```

### 3. æ¸¬è©¦å¤š GPU æ¨¡å¼

```bash
# 1. ä¸Šå‚³é•·éŸ³è¨Š (>5 åˆ†é˜)
# 2. **å‹¾é¸** Multi-GPU
# 3. é»æ“Š Start
# 4. ç¢ºèª 4 å¼µ GPU éƒ½æœ‰è² è¼‰
```

---

## ğŸ’¡ ç‚ºä»€éº¼ä¹‹å‰çš„æ–¹æ³•ä¸è¡Œï¼Ÿ

### faster-whisper çš„æ¶æ§‹

```
ä½ çš„ç¨‹å¼
    â†“
faster-whisper (Python)
    â†“
ctranslate2 (C++)
    â†“
CUDA (åº•å±¤)
```

**å•é¡Œ**ï¼š
- `ctranslate2` æ˜¯ C++ å¯¦ç¾çš„æ¨ç†å¼•æ“
- å®ƒçš„ device åƒæ•¸è¨­è¨ˆåªæ¥å— `"cuda"` æˆ– `"cpu"`
- ä¸åƒ PyTorch é‚£æ¨£æ”¯æ´ `"cuda:0"` æŒ‡å®šç‰¹å®š GPU

**è§£æ±º**ï¼š
- ä½¿ç”¨ PyTorch çš„ `torch.cuda.set_device(0)` åœ¨æ›´ä¸Šå±¤è¨­ç½®
- è®“åº•å±¤çš„æ‰€æœ‰ CUDA åº«éƒ½ä½¿ç”¨ GPU 0
- é€™æ˜¯æ¨™æº–ä¸”ç›¸å®¹çš„åšæ³•

---

## ğŸ“ ç›¸é—œæ–‡ä»¶

- âœ… `app.py` - å·²ä¿®æ”¹ï¼ˆv2 æ­£ç¢ºç‰ˆæœ¬ï¼‰
- âœ… `parallel_transcriber.py` - ä½¿ç”¨ç’°å¢ƒè®Šæ•¸æ§åˆ¶ï¼ˆæ­£ç¢ºï¼‰
- âœ… `transcriber.py` - ç„¡éœ€ä¿®æ”¹

---

## ğŸ‰ ç¸½çµ

### å•é¡Œ
`device="cuda:0"` å°è‡´ `ValueError: unsupported device cuda:0`

### è§£æ±ºæ–¹æ¡ˆ
ä½¿ç”¨ `torch.cuda.set_device(0)` + `device="cuda"`

### çµæœ
- âœ… å–® GPU æ¨¡å¼æ­£å¸¸å·¥ä½œï¼Œåªä½¿ç”¨ GPU 0
- âœ… å¤š GPU æ¨¡å¼æ­£å¸¸å·¥ä½œï¼Œä½¿ç”¨æ‰€æœ‰ GPU
- âœ… æ²’æœ‰éŒ¯èª¤è¨Šæ¯
- âœ… GPU æ§åˆ¶ç²¾ç¢º

---

**ç«‹å³é‡å»ºå®¹å™¨ï¼Œé€™æ¬¡æ‡‰è©²å®Œå…¨æ­£å¸¸äº†ï¼** ğŸš€

```bash
docker compose down && docker compose build && docker compose up -d
```
