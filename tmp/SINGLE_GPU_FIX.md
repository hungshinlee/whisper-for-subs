# ä¿®æ­£å–® GPU æ¨¡å¼ - åªä½¿ç”¨ç¬¬ä¸€å¼µ GPU

## ğŸ“‹ ä¿®æ”¹èªªæ˜

ç•¶å–æ¶ˆå‹¾é¸ã€ŒUse Multi-GPU Parallel Processingã€æ™‚ï¼Œç³»çµ±æ‡‰è©²åªä½¿ç”¨ç¬¬ä¸€å¼µ GPUï¼ˆGPU 0ï¼‰ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ GPUã€‚

## ğŸ”§ ä¿®æ”¹å…§å®¹

éœ€è¦ä¿®æ”¹ `app.py` ä¸­çš„å…©å€‹åœ°æ–¹ï¼š

### 1. ä¿®æ”¹ get_transcriber å‡½æ•¸

**åŸä»£ç¢¼ï¼ˆç´„ç¬¬ 115-127 è¡Œï¼‰ï¼š**
```python
def get_transcriber(
    model_size: str = "large-v3",
    use_vad: bool = True,
) -> WhisperTranscriber:
    """Get or create single-GPU transcriber instance."""
    global transcriber
    
    if transcriber is None or transcriber.model_size != model_size:
        transcriber = WhisperTranscriber(
            model_size=model_size,
            device=os.environ.get("WHISPER_DEVICE", "cuda"),
            compute_type=os.environ.get("WHISPER_COMPUTE_TYPE", "float16"),
            use_vad=use_vad,
        )
    
    return transcriber
```

**ä¿®æ”¹ç‚ºï¼š**
```python
def get_transcriber(
    model_size: str = "large-v3",
    use_vad: bool = True,
) -> WhisperTranscriber:
    """Get or create single-GPU transcriber instance (uses only GPU 0)."""
    global transcriber
    
    if transcriber is None or transcriber.model_size != model_size:
        # For single GPU mode, explicitly use only the first GPU (cuda:0)
        device = os.environ.get("WHISPER_DEVICE", "cuda")
        if device == "cuda":
            device = "cuda:0"  # Explicitly use GPU 0
        
        transcriber = WhisperTranscriber(
            model_size=model_size,
            device=device,
            compute_type=os.environ.get("WHISPER_COMPUTE_TYPE", "float16"),
            use_vad=use_vad,
        )
    
    return transcriber
```

### 2. ä¿®æ”¹ process_audio å‡½æ•¸ä¸­çš„æç¤ºä¿¡æ¯

åœ¨ process_audio å‡½æ•¸ä¸­ï¼ˆç´„ç¬¬ 250 è¡Œï¼‰ï¼Œå°‡ï¼š
```python
# Single GPU processing
yield format_progress_html(35, "Loading Whisper model..."), "", None
trans = get_transcriber(model_size, use_vad)

yield format_progress_html(40, "Model loaded. Starting transcription..."), "", None
```

ä¿®æ”¹ç‚ºï¼š
```python
# Single GPU processing (uses only GPU 0)
yield format_progress_html(35, "Loading Whisper model on GPU 0..."), "", None
trans = get_transcriber(model_size, use_vad)

yield format_progress_html(40, "Model loaded on GPU 0. Starting transcription..."), "", None
```

ä¸¦ä¿®æ”¹ç‹€æ…‹ä¿¡æ¯ï¼ˆç´„ç¬¬ 297 è¡Œï¼‰ï¼š
```python
# Format status message with duration and processing time
gpu_info = f"{num_gpus_used} GPUs" if use_parallel else "GPU 0 (single)"
```

---

## ğŸš€ å¿«é€Ÿä¿®å¾©

å®Œæ•´çš„ä¿®æ”¹å·²æº–å‚™åœ¨ï¼š`tmp/app_fixed.py`

éƒ¨ç½²æ–¹æ³•ï¼š
```bash
cd /Users/winston/Projects/whisper-for-subs

# å‚™ä»½
cp app.py app.py.backup_gpu

# éƒ¨ç½²ä¿®å¾©ç‰ˆæœ¬
cp tmp/app_fixed.py app.py

# é‡å»ºå®¹å™¨
docker compose down
docker compose build
docker compose up -d

# æŸ¥çœ‹æ—¥èªŒ
docker compose logs -f
```

---

## âœ… é æœŸæ•ˆæœ

### å–® GPU æ¨¡å¼ï¼ˆå–æ¶ˆå‹¾é¸ï¼‰
```
Loading Whisper model on GPU 0...
Model loaded on GPU 0. Starting transcription...
âœ… Transcription complete! 
Mode: GPU 0 (single)
```

### å¤š GPU æ¨¡å¼ï¼ˆå‹¾é¸ï¼‰
```
Starting parallel transcription on 4 GPUs...
[GPU 0] â–¶ Processing segment 0
[GPU 1] â–¶ Processing segment 1
[GPU 2] â–¶ Processing segment 2
[GPU 3] â–¶ Processing segment 3
âœ… Transcription complete!
Mode: 4 GPUs
```

---

## ğŸ” é©—è­‰æ–¹æ³•

### æ¸¬è©¦å–® GPU æ¨¡å¼

1. ä¸Šå‚³çŸ­éŸ³è¨Šï¼ˆ< 5 åˆ†é˜ï¼‰
2. **å–æ¶ˆå‹¾é¸** ã€ŒUse Multi-GPUã€
3. é»æ“Šã€ŒğŸš€ Startã€
4. ç›£æ§ GPU ä½¿ç”¨æƒ…æ³ï¼š

```bash
# åœ¨å¦ä¸€å€‹çµ‚ç«¯åŸ·è¡Œ
watch -n 1 nvidia-smi

# æ‡‰è©²åªçœ‹åˆ° GPU 0 åœ¨ä½¿ç”¨
# GPU 1, 2, 3 æ‡‰è©²é–’ç½®
```

### æ¸¬è©¦å¤š GPU æ¨¡å¼

1. ä¸Šå‚³é•·éŸ³è¨Šï¼ˆ> 5 åˆ†é˜ï¼‰
2. **å‹¾é¸** ã€ŒUse Multi-GPUã€
3. é»æ“Šã€ŒğŸš€ Startã€
4. æ‡‰è©²çœ‹åˆ° 4 å¼µ GPU éƒ½åœ¨å·¥ä½œ

---

## ğŸ“Š ä¿®æ”¹å‰å¾Œå°æ¯”

| æ¨¡å¼ | ä¿®æ”¹å‰ | ä¿®æ”¹å¾Œ |
|-----|--------|--------|
| å–® GPUï¼ˆå–æ¶ˆå‹¾é¸ï¼‰| å¯èƒ½ä½¿ç”¨å¤šå¼µ GPU âŒ | åªä½¿ç”¨ GPU 0 âœ… |
| å¤š GPUï¼ˆå‹¾é¸ï¼‰| ä½¿ç”¨ 4 å¼µ GPU âœ… | ä½¿ç”¨ 4 å¼µ GPU âœ… |
| æç¤ºä¿¡æ¯ | ä¸æ˜ç¢º | æ¸…æ¥šæ¨™ç¤ºä½¿ç”¨çš„ GPU |

---

## ğŸ’¡ æŠ€è¡“èªªæ˜

### ç‚ºä»€éº¼éœ€è¦ cuda:0

åœ¨ PyTorch å’Œ faster-whisper ä¸­ï¼š
- `device="cuda"` - ä½¿ç”¨é è¨­ GPUï¼ˆé€šå¸¸æ˜¯ GPU 0ï¼Œä½†ä¸ä¿è­‰ï¼‰
- `device="cuda:0"` - **æ˜ç¢ºä½¿ç”¨ GPU 0**
- `device="cuda:1"` - æ˜ç¢ºä½¿ç”¨ GPU 1

ç•¶ `CUDA_VISIBLE_DEVICES=0,1,2,3` æ™‚ï¼Œæ‰€æœ‰ 4 å¼µ GPU éƒ½å¯è¦‹ï¼Œæ‰€ä»¥éœ€è¦æ˜ç¢ºæŒ‡å®š `cuda:0` ä¾†ç¢ºä¿åªä½¿ç”¨ç¬¬ä¸€å¼µã€‚

### å¤š GPU æ¨¡å¼å¦‚ä½•å·¥ä½œ

å¤š GPU æ¨¡å¼ä½¿ç”¨ `CUDA_VISIBLE_DEVICES` ç’°å¢ƒè®Šæ•¸åœ¨**æ¯å€‹å­é€²ç¨‹ä¸­**æ§åˆ¶å¯è¦‹çš„ GPUï¼š
- å­é€²ç¨‹ 1: `CUDA_VISIBLE_DEVICES=0` â†’ åªçœ‹åˆ° GPU 0
- å­é€²ç¨‹ 2: `CUDA_VISIBLE_DEVICES=1` â†’ åªçœ‹åˆ° GPU 1
- å­é€²ç¨‹ 3: `CUDA_VISIBLE_DEVICES=2` â†’ åªçœ‹åˆ° GPU 2
- å­é€²ç¨‹ 4: `CUDA_VISIBLE_DEVICES=3` â†’ åªçœ‹åˆ° GPU 3

é€™æ¨£æ¯å€‹é€²ç¨‹éƒ½ç¨ç«‹ä½¿ç”¨ä¸€å¼µ GPUï¼Œå¯¦ç¾ä¸¦è¡Œè™•ç†ã€‚
